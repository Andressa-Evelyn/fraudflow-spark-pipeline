# FraudFlow ‚Äì Data Pipeline de Detec√ß√£o de Anomalias com Spark e Airflow

Este projeto implementa um **pipeline de dados para detec√ß√£o de anomalias (fraudes)** utilizando **Apache Spark** para processamento distribu√≠do, **Apache Airflow** para orquestra√ß√£o e **Docker** para padroniza√ß√£o e execu√ß√£o do ambiente.

O objetivo √© demonstrar, de forma pr√°tica, como integrar ferramentas amplamente usadas em engenharia de dados em um ambiente reproduz√≠vel e organizado.

---

## üõ†Ô∏è Tecnologias Utilizadas

- **Apache Spark**  
  Processamento distribu√≠do de dados e transforma√ß√µes em larga escala.

- **Apache Airflow**  
  Orquestra√ß√£o do pipeline, controle de depend√™ncias e agendamento das tarefas.

- **Docker & Docker Compose**  
  Containeriza√ß√£o e orquestra√ß√£o dos servi√ßos, garantindo ambiente consistente.

- **Python (PySpark)**  
  Implementa√ß√£o das regras de neg√≥cio e transforma√ß√µes dos dados.

---

## üèóÔ∏è Arquitetura do Projeto

O pipeline segue uma arquitetura simples e did√°tica, focada em boas pr√°ticas de engenharia de dados:

1. **Airflow** √© respons√°vel por orquestrar o fluxo de execu√ß√£o.
2. O **DAG do Airflow** dispara jobs do **Spark**.
3. O **Spark** l√™ dados brutos (raw), processa e aplica regras de transforma√ß√£o.
4. Os dados transformados s√£o gravados em uma camada intermedi√°ria (staging/processed).
5. Todo o ambiente roda de forma isolada via **Docker Compose**.

---

## ‚ñ∂Ô∏è Como Executar o Projeto Localmente

### Pr√©-requisitos

- Docker
- Docker Compose

> N√£o √© necess√°rio instalar Spark ou Airflow localmente.

---

### Passo a passo

   ```bash
   git clone https://github.com/Andressa-Evelyn/fraudflow-spark-pipeline.git
   cd fraudflow-spark-pipeline
   docker compose up -d
   docker ps
   acesse o Airflow
   ative o DAG no painel do Airflow



